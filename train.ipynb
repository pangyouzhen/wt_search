{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/new.train.query.txt\",sep=\"\\t\",names=[\"query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>美赞臣亲舒一段</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>慱朗手动料理机</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>電力貓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>掏夹缝工具</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>飞推vip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     query\n",
       "1  美赞臣亲舒一段\n",
       "2  慱朗手动料理机\n",
       "3      電力貓\n",
       "4    掏夹缝工具\n",
       "5    飞推vip"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = './model'\n",
    "# 读取模型对应的tokenizer\n",
    "tokenizer: BertTokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# 载入模型\n",
    "model: BertModel = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoding={'input_ids': [[101, 791, 1921, 1921, 3698, 2523, 1962, 1557, 117, 872, 1962, 1408, 102], [101, 3152, 3315, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "len(input_ids)=13\n",
      "input_ids=[101, 791, 1921, 1921, 3698, 2523, 1962, 1557, 117, 872, 1962, 1408, 102]\n",
      "torch.Size([1, 13])\n",
      "batch_encoding['input_ids']=[[101, 791, 1921, 1921, 3698, 2523, 1962, 1557, 117, 872, 1962, 1408, 102], [101, 3152, 3315, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "False\n",
      "tensor([[[ 1.0728e+00,  1.0669e-02,  5.0070e-01,  ...,  2.5274e-01,\n",
      "           3.0154e-01, -5.1296e-01],\n",
      "         [-8.5284e-04,  3.8647e-01,  7.8298e-01,  ..., -1.1066e+00,\n",
      "          -2.7329e-01, -1.2681e-01],\n",
      "         [ 1.0458e+00, -1.1362e-01, -3.1574e-01,  ...,  2.0175e-02,\n",
      "           8.6139e-01,  3.3297e-02],\n",
      "         ...,\n",
      "         [ 1.4743e+00, -4.7688e-01,  5.8316e-01,  ..., -1.4346e-01,\n",
      "           9.3235e-01, -6.3538e-01],\n",
      "         [ 1.4120e+00, -1.3553e-01,  6.1539e-01,  ...,  1.9678e-01,\n",
      "           8.7175e-01, -3.9543e-01],\n",
      "         [ 1.9585e-01,  4.4761e-02,  2.1270e-01,  ..., -2.5068e-01,\n",
      "           8.1684e-01, -5.3060e-01]],\n",
      "\n",
      "        [[-1.0120e+00,  3.2229e-01, -5.9354e-01,  ...,  1.1258e+00,\n",
      "          -3.6925e-01, -1.4669e-02],\n",
      "         [-7.0769e-01,  2.4443e-01, -7.4587e-02,  ..., -1.7254e-01,\n",
      "          -2.1717e-01, -1.2404e-01],\n",
      "         [-2.9629e-01,  3.2810e-01,  5.0813e-01,  ..., -3.9465e-02,\n",
      "          -2.7750e-01, -2.0076e-01],\n",
      "         ...,\n",
      "         [ 2.3015e-01,  2.3701e-01, -7.2276e-01,  ...,  9.8289e-01,\n",
      "          -6.0453e-01, -4.5077e-02],\n",
      "         [ 1.1630e-01,  2.0993e-01, -7.3641e-01,  ...,  9.3215e-01,\n",
      "          -6.9120e-01, -1.6686e-01],\n",
      "         [-7.0283e-02,  2.0578e-01, -7.8145e-01,  ...,  9.6363e-01,\n",
      "          -6.6622e-01, -1.2492e-01]]])\n",
      "torch.Size([2, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"今天天气很好啊,你好吗\"\n",
    "# todo input_text2 怎样一起输入\n",
    "input_text2 = \"文本\"\n",
    "# 通过tokenizer把文本变成 token_id\n",
    "batch_encoding: BatchEncoding = tokenizer([input_text,input_text2],padding=True)\n",
    "print(f\"{batch_encoding=}\")\n",
    "input_ids: List[int] = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "print(f\"{len(input_ids)=}\")\n",
    "print(f\"{input_ids=}\")\n",
    "# 101 代表cls 符号，102代表 sep\n",
    "# [101, 791, 1921, 1921, 3698, 2523, 1962, 1557, 117, 872, 1962, 1408, 102]\n",
    "input_ids: Tensor = torch.tensor([input_ids])\n",
    "# 获得BERT模型最后一个隐层结果\n",
    "print(input_ids.shape)\n",
    "print(f\"{batch_encoding['input_ids']=}\")\n",
    "with torch.no_grad():\n",
    "    # bert 的输入为 batch_size,seq_length\n",
    "    a = model(torch.tensor(batch_encoding['input_ids']))\n",
    "    # a = model(input_ids)\n",
    "    last_hidden_state = a.last_hidden_state\n",
    "    # batch_size,seq_length,embedding\n",
    "    print(torch.equal(last_hidden_state[:, 0, :], a.pooler_output))\n",
    "    # last_hidden_state 的输出为 batch_size,seq_length, embedding\n",
    "    print(last_hidden_state)\n",
    "    print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.MaxPool1d(3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 74])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((input,output),dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 24])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SentenceBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__\n",
    "\n",
    "    def forward(self,sen_a,sentb):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(BertTokenizer.__call__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'> (self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      "<class 'transformers.tokenization_utils.PreTrainedTokenizer'> (self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      "<class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'> (self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      "<class 'transformers.tokenization_utils_base.SpecialTokensMixin'> (*args, **kwargs)\n",
      "<class 'transformers.file_utils.PushToHubMixin'> (*args, **kwargs)\n",
      "<class 'object'> (*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "for i in BertTokenizer.__mro__:\n",
    "    print(i,inspect.signature(i.__call__))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
